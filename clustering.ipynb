{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed18269-9962-41f2-beb3-45893a3ad8f9",
   "metadata": {},
   "source": [
    "To do:\n",
    "- Compute validity indexes with the gower / hamming distance: done for silhouette, not possible for the others?\n",
    "- Implement preprocessing with minmax and compare results with normalization\n",
    "- Store the validity index optimized by each best model\n",
    "- Use gap statistics instead of elbow method for model selection\n",
    "- Compute relative fit criteria for latent models\n",
    "- Adapt kmeans and AHC to use Mahalanobis distance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676fe747-bd8b-4115-83bc-0a18c2c9cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torchmetrics\n",
    "# pip install stepmix\n",
    "# pip install kneed\n",
    "# pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4887e505-68fb-40c0-adb5-06ef3199e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from joblib import Parallel, delayed # for parallelization\n",
    "from itertools import product\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "\n",
    "# Clustering\n",
    "from stepmix.stepmix import StepMix\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import AgglomerativeClustering, HDBSCAN\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.neighbors import BallTree\n",
    "import torch\n",
    "from torchmetrics.clustering import DunnIndex\n",
    "from collections import Counter\n",
    "from kneed import KneeLocator\n",
    "\n",
    "# Visualization\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import ConvexHull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d1e61c-3574-4482-9529-59268b7fb8d6",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1190dd79-e4b5-4684-98b0-0f6ce7797686",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2004_i = pd.read_parquet(\"data/data2004_i.parquet\") # load imputed data\n",
    "\n",
    "# Dataset with numeric outcomes\n",
    "data_n = data2004_i[[\n",
    "    # Q2\n",
    "    'clseusa_n', # 'clsetown_n', 'clsestat_n', 'clsenoam_n',\n",
    "    # Q3\n",
    "    'ambornin_n', 'amcit_n', 'amlived_n', 'amenglsh_n', \n",
    "    'amchrstn_n', 'amgovt_n', 'amfeel_n', # 'amancstr_n',\n",
    "    # Q4\n",
    "    'amcitizn_n', 'amshamed_n', 'belikeus_n', 'ambetter_n', 'ifwrong_n', # 'amsports_n', 'lessprd_n',\n",
    "    # Q5\n",
    "    'proudsss_n', 'proudgrp_n', 'proudpol_n', 'prouddem_n', 'proudeco_n',\n",
    "    'proudspt_n', 'proudart_n', 'proudhis_n', 'proudmil_n', 'proudsci_n']]\n",
    "\n",
    "## Scaling and normalizing\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "data_n_scaled = scaler.fit_transform(data_n)\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "data_n_norm = normalizer.fit_transform(data_n)\n",
    "\n",
    "# Dataset with categorical outcomes\n",
    "data_f = data2004_i[[\n",
    "    # Q2\n",
    "    'clseusa_f', # 'clsetown_f', 'clsestat_f', 'clsenoam_f',\n",
    "    # Q3\n",
    "    'ambornin_f', 'amcit_f', 'amlived_f', 'amenglsh_f', \n",
    "    'amchrstn_f', 'amgovt_f', 'amfeel_f', # 'amancstr_f',\n",
    "    # Q4\n",
    "    'amcitizn_f', 'amshamed_f', 'belikeus_f', 'ambetter_f', 'ifwrong_f', # 'amsports_f', 'lessprd_f',\n",
    "    # Q5\n",
    "    'proudsss_f', 'proudgrp_f', 'proudpol_f', 'prouddem_f', 'proudeco_f',\n",
    "    'proudspt_f', 'proudart_f', 'proudhis_f', 'proudmil_f', 'proudsci_f']]\n",
    "\n",
    "## One-hot encoding\n",
    "data_f_oh = data_f.apply(lambda col: LabelEncoder().fit_transform(col))\n",
    "\n",
    "# Dataset with controls\n",
    "controls = data2004_i[[\n",
    "    'sex', 'race_f', 'born_usa', 'party_fs', 'religstr_f', \n",
    "    'reltrad_f', 'region_f']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b97bf-43e0-4516-b3ee-f226e4ebd772",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b24ac-f239-4dea-9283-e3491ec65ee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_clust = 12\n",
    "max_threads = 8\n",
    "\n",
    "val_indexes = ['silhouette_euclid','silhouette_hamm' , 'calinski_harabasz', 'davies_bouldin', 'dunn', 'inertia']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d170e0b-2c98-40ec-bda1-12743bfd5b9e",
   "metadata": {},
   "source": [
    "## Validity indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c84df2-d928-437c-b4f8-200cc8b0d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom score functions to avoid throwing errors when undefined\n",
    "def sil_score(data, pred_clust):\n",
    "    try:\n",
    "        sil_score_euclidean = silhouette_score(data, pred_clust, metric='euclidean')\n",
    "    except ValueError:\n",
    "        sil_score_euclidean = np.nan\n",
    "\n",
    "    try:\n",
    "        sil_score_hamming = silhouette_score(data_f_oh, pred_clust, metric='hamming')\n",
    "    except ValueError:\n",
    "        sil_score_hamming = np.nan\n",
    "\n",
    "    return sil_score_euclidean, sil_score_hamming\n",
    "\n",
    "def ch_score(data, pred_clust):\n",
    "    try:\n",
    "        ch_score = calinski_harabasz_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        ch_score = np.nan\n",
    "    return ch_score\n",
    "\n",
    "def db_score(data, pred_clust):\n",
    "    try:\n",
    "        db_score = davies_bouldin_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        db_score = np.nan\n",
    "    return db_score\n",
    "\n",
    "def dunn_score(data, pred_clust):\n",
    "    torch_data = np.array(data)\n",
    "    torch_data = torch.tensor(torch_data, dtype=torch.float32)\n",
    "    torch_pred_clust = torch.tensor(pred_clust, dtype=torch.int64)\n",
    "\n",
    "    dunn_metric = DunnIndex()\n",
    "    \n",
    "    try:\n",
    "        dunn_score = float(dunn_metric(torch_data, torch_pred_clust))\n",
    "    except Exception:\n",
    "        dunn_score = np.nan\n",
    " \n",
    "    return dunn_score\n",
    "\n",
    "def inertia(data, labels):\n",
    "    data = np.asarray(data)\n",
    "    \n",
    "    inertia = 0\n",
    "    for cluster in np.unique(labels):\n",
    "        cluster_points = data[labels == cluster]\n",
    "        cluster_centroid = np.mean(cluster_points, axis=0)\n",
    "        inertia += np.sum((cluster_points - cluster_centroid) ** 2)\n",
    "        \n",
    "    return inertia\n",
    "\n",
    "def clust_size(labels):\n",
    "    cluster_sizes = Counter(labels)\n",
    "    min_size = min(cluster_sizes.values())\n",
    "    max_size = max(cluster_sizes.values())\n",
    "    \n",
    "    return min_size, max_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf65c0e6-238a-4879-87e3-074f85d8333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return all validity indexes at once\n",
    "def get_metrics(model, params, n, data, pred_clust, **additional_metrics):\n",
    "    base_metrics = {\n",
    "        'model': model,\n",
    "        'params': params,\n",
    "        'n_clust': n,\n",
    "        'min_clust_size': clust_size(pred_clust)[0],\n",
    "        'max_clust_size': clust_size(pred_clust)[1],\n",
    "        'silhouette_euclid': sil_score(data, pred_clust)[0],\n",
    "        'silhouette_hamm': sil_score(data, pred_clust)[1],\n",
    "        'calinski_harabasz': ch_score(data, pred_clust),\n",
    "        'davies_bouldin': db_score(data, pred_clust),\n",
    "        'dunn': dunn_score(data, pred_clust),\n",
    "        'inertia': inertia(data, pred_clust)\n",
    "    }\n",
    "\n",
    "    base_metrics.update(additional_metrics)\n",
    "    return base_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7af8aaa-6bf2-4520-a53c-3850fc38cdb6",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabf9fd9-5fbf-4b62-aa77-f9b4e5a00284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display the optimal numbers of clutsters according to each validity index\n",
    "def elbow_plot(df, val_index):\n",
    "    res = df.dropna(subset=[val_index])\n",
    "\n",
    "    x = res[\"n_clust\"]\n",
    "    y = res[val_index]\n",
    "\n",
    "    if val_index in ['davies_bouldin', 'entropy']:\n",
    "        knee_locator = KneeLocator(x, y, curve='concave', direction='increasing')\n",
    "    else:\n",
    "        knee_locator = KneeLocator(x, y, curve='convex', direction='decreasing')\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(x, y, marker=\"o\", linestyle=\"-\", label=val_index)\n",
    "    plt.axvline(x=knee_locator.knee, color=\"r\", linestyle=\"--\", label=f\"Optimal k={knee_locator.knee}\")\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(f\"{val_index} index\")\n",
    "    plt.title(f\"Elbow Method for {val_index} index\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694ebf46-f7f9-49b9-a9bd-adbefb54769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot datapoints and clusters\n",
    "def plot_clusters(data, clust_range, pred_clust):\n",
    "    \n",
    "    # PCA to define the 2D space\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_space = pca.fit_transform(data_n)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Collect all hull vertices\n",
    "    hull_vertices = []\n",
    "    hull_colors = []\n",
    "    for i in clust_range:\n",
    "        cluster_points = reduced_space[pred_clust == i]\n",
    "        if len(cluster_points) > 2:\n",
    "            hull = ConvexHull(cluster_points)\n",
    "            hull_vertices.append((\n",
    "                cluster_points[hull.vertices, 0],\n",
    "                cluster_points[hull.vertices, 1]\n",
    "            ))\n",
    "            hull_colors.append(i)\n",
    "\n",
    "    # Plot datapoints\n",
    "    scatter = plt.scatter(reduced_space[:, 0], reduced_space[:, 1], \n",
    "                         c=pred_clust, cmap='tab10', \n",
    "                         s=15, edgecolors='k')\n",
    "\n",
    "    # Plot all hulls using the same colormap\n",
    "    for vertices, i in zip(hull_vertices, hull_colors):\n",
    "        plt.fill(vertices[0], vertices[1], \n",
    "                 alpha=0.3,\n",
    "                 color=scatter.cmap(scatter.norm(i)))\n",
    "\n",
    "    legend = plt.legend(*scatter.legend_elements())\n",
    "    plt.xlabel(\"Dim 1\")\n",
    "    plt.ylabel(\"Dim 2\")\n",
    "    plt.title(\"Clusters with Convex Hulls\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e3fb34-0690-447b-a4bd-2df51e32e567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot response patterns\n",
    "def plot_cluster_profiles(features, \n",
    "                          cluster_labels, \n",
    "                          feature_names, \n",
    "                          class_names,\n",
    "                          sd,\n",
    "                          alpha=0.4):\n",
    "    \"\"\"\n",
    "    Create a profile plot for clustering results, supporting both probabilistic\n",
    "    (e.g., LCA, GMM) and deterministic (e.g., k-means) clustering methods.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    features : array-like or pandas.DataFrame\n",
    "        The original feature matrix used for clustering (n_samples, n_features)\n",
    "    cluster_labels : array-like\n",
    "        Cluster assignments for each sample (n_samples,)\n",
    "    feature_names : list, optional\n",
    "        Names of the features (default: None, will use indices or DataFrame columns)\n",
    "    class_names : list, optional\n",
    "        Names of the classes (default: None, will use indices)\n",
    "    sd : float\n",
    "        Number of standard deviations around the mean to plot\n",
    "    alpha : float, optional\n",
    "        Base transparency for the scatter points\n",
    "    \"\"\"\n",
    "    # Convert features to numpy array if it's a DataFrame\n",
    "    if isinstance(features, pd.DataFrame):\n",
    "        if feature_names is None:\n",
    "            feature_names = features.columns.tolist()\n",
    "        features = features.to_numpy()\n",
    "    \n",
    "    # Convert cluster_labels to numpy array if it's a Series\n",
    "    if isinstance(cluster_labels, pd.Series):\n",
    "        cluster_labels = cluster_labels.to_numpy()\n",
    "    \n",
    "    # Handle NaN values\n",
    "    features = np.nan_to_num(features, nan=np.nanmean(features))\n",
    "    \n",
    "    n_features = features.shape[1]\n",
    "    n_classes = len(np.unique(cluster_labels))\n",
    "    \n",
    "    if feature_names is None:\n",
    "        feature_names = [f'Feature {i+1}' for i in range(n_features)]\n",
    "    if class_names is None:\n",
    "        class_names = [f'Class {i+1}' for i in range(n_classes)]\n",
    "        \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12,6))\n",
    "    \n",
    "    # Calculate class centroids and confidence intervals\n",
    "    centroids = []\n",
    "    std_devs = []\n",
    "    \n",
    "    for class_idx in range(n_classes):\n",
    "        class_mask = cluster_labels == class_idx\n",
    "        class_data = features[class_mask]\n",
    "        \n",
    "        if len(class_data) > 0:\n",
    "            # Calculate centroid\n",
    "            centroid = np.nanmean(class_data, axis=0)\n",
    "            centroids.append(centroid)\n",
    "            \n",
    "            # Calculate standard deviations\n",
    "            std_dev = np.nanstd(class_data, axis=0)\n",
    "            std_devs.append(std_dev)\n",
    "            \n",
    "        else:\n",
    "            # Handle empty classes\n",
    "            centroids.append(np.zeros(n_features))\n",
    "            std_devs.append(np.zeros(n_features))\n",
    "    \n",
    "    # Convert to numpy arrays for vectorized operations\n",
    "    centroids = np.array(centroids)\n",
    "    std_devs = np.array(std_devs)\n",
    "    \n",
    "    # Plot for each class\n",
    "    x = np.arange(n_features)\n",
    "    width = 0.8 / n_classes\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        # Offset x positions for each class\n",
    "        x_pos = x - (width * (n_classes-1)/2) + (i * width)\n",
    "        \n",
    "        # Plot standard deviation boxes\n",
    "        for j in range(n_features):\n",
    "            # Clamp values to [-2, 2] range\n",
    "           lower = max(-2, centroids[i][j] - std_devs[i][j]*sd/2)\n",
    "           upper = min(2, centroids[i][j] + std_devs[i][j]*sd/2)\n",
    "           height = upper - lower\n",
    "       \n",
    "           rect = plt.Rectangle((x_pos[j] - width/2, lower),\n",
    "                          width, height,\n",
    "                          alpha=0.2, color=f'C{i}')\n",
    "           ax.add_patch(rect)   \n",
    "        \n",
    "        # Plot centroids\n",
    "        ax.scatter(x_pos, centroids[i], color=f'C{i}', \n",
    "                  label=class_names[i], marker='*', zorder=5)\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(feature_names, rotation=45, ha='right')\n",
    "    ax.set_ylabel('Answers')\n",
    "    ax.legend(title='Clusters')\n",
    "    ax.grid(True, axis='y', alpha=0.3)\n",
    "    ax.axhline(y=0, color='grey', linestyle='dashed', linewidth=1)\n",
    "    ax.set_title(f\"Cluster Profile Plot (mean Â± {sd} standard deviation)\")\n",
    "    plt.tight_layout()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1513b5bc-963b-41e5-a6bd-12f45f22c2f5",
   "metadata": {},
   "source": [
    "# Latent models\n",
    "With the StepMix package, see: https://github.com/Labo-Lacourse/stepmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78aabed-4cb1-4e87-a35f-797240d34b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "clust_range = range(1, max_clust+1)\n",
    "\n",
    "opt_params = {\n",
    "    'method': 'gradient',\n",
    "    'intercept': True,\n",
    "    'max_iter': 2500,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa743e87-3ef4-4e07-9610-59a15540bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit models without covariates\n",
    "def do_StepMix(n, type, data):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "        latent_mod = StepMix(\n",
    "            n_components = n, \n",
    "            measurement = type, \n",
    "            n_init = 3,\n",
    "            init_params = 'kmeans',\n",
    "            structural_params = opt_params,\n",
    "            random_state = 123)\n",
    "        \n",
    "        latent_mod.fit(data)\n",
    "        pred_clust = latent_mod.predict(data)\n",
    "\n",
    "        model = 'LCA' if type == 'categorical' else 'LPA'\n",
    "        params = 'without covariates'\n",
    "        loglik = latent_mod.score(data)\n",
    "        aic = latent_mod.aic(data)\n",
    "        bic = latent_mod.aic(data)\n",
    "        entropy = latent_mod.entropy(data)\n",
    "\n",
    "    return get_metrics(model, params, n, data, pred_clust, LL = loglik, aic = aic, bic = bic, entropy = entropy)\n",
    "\n",
    "cat_results = Parallel(n_jobs=8)(delayed(do_StepMix)(n, 'categorical', data_f_oh) for n in clust_range)\n",
    "LCA_all = pd.DataFrame(cat_results)\n",
    "\n",
    "num_results = Parallel(n_jobs=8)(delayed(do_StepMix)(n, 'continuous', data_n_scaled) for n in clust_range)\n",
    "LPA_all = pd.DataFrame(num_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d177789-db00-4153-8227-5ec8932dd193",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val_index in val_indexes + ['aic', 'bic', 'entropy']:\n",
    "    elbow_plot(LCA_all, val_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1461c4-fd28-4831-96e2-c4edc004c06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit models with covariates\n",
    "def do_StepMix_covar(n, type, data):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "        \n",
    "        latent_mod = StepMix(\n",
    "            n_components = n,\n",
    "            measurement = type,\n",
    "            n_init = 3,\n",
    "            init_params = 'kmeans',\n",
    "            structural = 'covariate', \n",
    "            n_steps = 1,\n",
    "            structural_params = opt_params,\n",
    "            random_state = 123)\n",
    "        \n",
    "        latent_mod.fit(data, controls_dum)\n",
    "        pred_clust = latent_mod.predict(data)\n",
    "        \n",
    "        model = 'LCA' if type == 'categorical' else 'LPA'\n",
    "        params = 'with covariates'\n",
    "        loglik = latent_mod.score(data)\n",
    "        aic = latent_mod.aic(data)\n",
    "        bic = latent_mod.aic(data)\n",
    "        entropy = latent_mod.entropy(data)\n",
    "\n",
    "    return get_metrics(model, params, n, data, pred_clust, LL = loglik, aic = aic, bic = bic, entropy = entropy)\n",
    "\n",
    "controls_dum = pd.get_dummies(controls)\n",
    "\n",
    "cat_results = Parallel(n_jobs=max_threads)(delayed(do_StepMix_covar)(n, 'categorical', data_f_oh) for n in clust_range)\n",
    "LCA_covar_all = pd.DataFrame(cat_results)\n",
    "\n",
    "# Data preprocessing?\n",
    "num_results = Parallel(n_jobs=max_threads)(delayed(do_StepMix_covar)(n, 'continuous', data_n_scaled) for n in clust_range)\n",
    "LPA_covar_all = pd.DataFrame(num_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d8e1dc-5d1b-415a-b51f-a7b75be5ca46",
   "metadata": {},
   "source": [
    "## Best latent models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6a8b3-eaad-4b81-a4cf-19367debf6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to select models based on aic / bic: using their absolute minimum, or an elbow method?\n",
    "# Absolute minimum yields the model with the most classes, so not appropriate\n",
    "LCA_aic_min = LCA_all.sort_values('aic', ascending=True).iloc[0]\n",
    "LCA_bic_min = LCA_all.sort_values('bic', ascending=True).iloc[0]\n",
    "\n",
    "LPA_aic_min = LPA_all.sort_values('aic', ascending=True).iloc[0]\n",
    "LPA_bic_min = LPA_all.sort_values('bic', ascending=True).iloc[0]\n",
    "\n",
    "abs_fit = pd.DataFrame([LCA_aic_min, LCA_bic_min, LPA_aic_min, LPA_bic_min])\n",
    "abs_fit = abs_fit.drop_duplicates().reset_index(drop=True)\n",
    "abs_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50aa4fe-c8bf-490b-a51e-cb3287f70b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best models according to relative fit = LRT / BLRT / BVR (LCA only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9fa7ce-84ae-4f7c-818e-aec8cb2c57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model for each combination of parameters through the Elbow method\n",
    "def elbow_method(df, val_index):\n",
    "    res = df.dropna(subset=[val_index])\n",
    "\n",
    "    x = res[\"n_clust\"]\n",
    "    y = res[val_index]\n",
    "\n",
    "    if val_index in ['davies_bouldin', 'entropy']:\n",
    "        knee_locator = KneeLocator(x, y, curve='concave', direction='increasing')\n",
    "    else:\n",
    "        knee_locator = KneeLocator(x, y, curve='convex', direction='decreasing')\n",
    "    \n",
    "    return res[res[\"n_clust\"] == knee_locator.knee]\n",
    "\n",
    "models = [LCA_all, LPA_all] # + [LCA_covar_all, LPA_covar_all]\n",
    "\n",
    "params = product(models, val_indexes + ['aic', 'bic', 'entropy'])\n",
    "\n",
    "latent_elbow = pd.DataFrame()\n",
    "for model, val_index in params:\n",
    "    best_model = elbow_method(model, val_index)\n",
    "    latent_elbow = pd.concat([latent_elbow, best_model], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a49207-56b4-4705-9a3a-b8afe7d740b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find absolute best models for each validity index\n",
    "latent_elbow = latent_elbow.drop_duplicates().reset_index(drop=True)\n",
    "# Need to add colums indicating which validity index is maximized.\n",
    "# After that, duplicate models should be merged, not dropped.\n",
    "\n",
    "best_silhouette_euclid = latent_elbow.sort_values('silhouette_euclid', ascending=False).iloc[0]\n",
    "best_silhouette_hamm = latent_elbow.sort_values('silhouette_hamm', ascending=False).iloc[0]\n",
    "best_ch = latent_elbow.sort_values('calinski_harabasz', ascending=False).iloc[0]\n",
    "best_db = latent_elbow.sort_values('davies_bouldin', ascending=True).iloc[0]\n",
    "best_dunn = latent_elbow.sort_values('dunn', ascending=False).iloc[0]\n",
    "best_inertia = latent_elbow.sort_values('inertia', ascending=False).iloc[0]\n",
    "\n",
    "best_aic = latent_elbow.sort_values('aic', ascending=True).iloc[0]\n",
    "best_bic = latent_elbow.sort_values('bic', ascending=True).iloc[0]\n",
    "best_entropy = latent_elbow.sort_values('entropy', ascending=False).iloc[0]\n",
    "\n",
    "latent_best = pd.DataFrame([best_silhouette_euclid, best_silhouette_hamm, best_ch, best_db, best_dunn, best_inertia])\n",
    "latent_best = latent_best.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa6a49d-a534-4240-a6ee-c0a08e48e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f439f8-bcc6-4251-a179-ba8dc0bdfa7a",
   "metadata": {},
   "source": [
    "The inclusion of covariates makes almost no difference.\n",
    "\n",
    "All selected models have 3-4 clusters\n",
    "\n",
    "The best model overall seems to be the LPA one.\n",
    "- It has lower entropy, meaning it classifies the individuals with better certainty.\n",
    "- It has lower aic and bic, meaning better model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1714e7-27fd-4c60-be76-adb1962f1888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit the best model and display coefficients\n",
    "data = data_n\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "        \n",
    "    latent_mod = StepMix(\n",
    "        n_components = 3,\n",
    "        measurement = 'continuous',\n",
    "        n_init = 3,\n",
    "        init_params = 'kmeans',\n",
    "        structural_params = opt_params,\n",
    "        random_state = 123,\n",
    "        progress_bar = 0,\n",
    "        verbose = 1)\n",
    "        \n",
    "    latent_mod.fit(data)\n",
    "    pred_clust = latent_mod.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a95d4e3-98fd-42e8-b7f0-20f1bd261104",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(data, range(latent_mod.n_components), pred_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c531324a-aa46-4cd7-930b-0916f4e6b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response patterns\n",
    "fig, ax = plot_cluster_profiles(data_n, pred_clust, feature_names = None, class_names = None, sd = 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d893bba-c892-432d-9cb0-85d85acd6304",
   "metadata": {},
   "source": [
    "# K-means\n",
    "No available implementation allows to chose the distance metric and linkage function. Hence a custom class is proposed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7124b656-780a-4568-ab74-141c4f7f55cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleKMeans:\n",
    "    \"\"\"\n",
    "    K-Means implementation supporting different distance metrics and center computation methods.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_clusters : int\n",
    "        Number of clusters\n",
    "    metric : str, default='euclidean'\n",
    "        Distance metric: 'euclidean', 'manhattan', 'chebyshev'\n",
    "    center_method : str, default='mean'\n",
    "        Method to compute cluster centers: 'mean', 'median', 'medoid'\n",
    "    max_iter : int, default=100\n",
    "        Maximum number of iterations\n",
    "    n_init : int, default=10\n",
    "        Number of times the k-means algorithm will be run with different centroid seeds.\n",
    "        The final result will be the best output of n_init consecutive runs in terms of inertia.\n",
    "    random_state : int or None, default=None\n",
    "        Random state for reproducibility\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_clusters, metric='euclidean', center_method='mean', \n",
    "                 max_iter=100, n_init=10, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.metric = metric\n",
    "        self.center_method = center_method\n",
    "        self.max_iter = max_iter\n",
    "        self.n_init = n_init\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Define mapping from user-friendly names to scipy metrics\n",
    "        self.metric_mapping = {\n",
    "            'euclidean': 'euclidean',\n",
    "            'manhattan': 'cityblock',\n",
    "            'chebyshev': 'chebyshev'\n",
    "        }\n",
    "        \n",
    "        # Validate inputs\n",
    "        valid_metrics = list(self.metric_mapping.keys())\n",
    "        if metric not in valid_metrics:\n",
    "            raise ValueError(f\"metric must be one of {valid_metrics}\")\n",
    "            \n",
    "        valid_centers = ['mean', 'median', 'medoid']\n",
    "        if center_method not in valid_centers:\n",
    "            raise ValueError(f\"center_method must be one of {valid_centers}\")\n",
    "            \n",
    "        if self.n_init <= 0:\n",
    "            raise ValueError(\"n_init should be > 0\")\n",
    "    \n",
    "    def _compute_distances(self, X, centers):\n",
    "        \"\"\"Compute distances between points and centers using specified metric.\"\"\"\n",
    "        return cdist(X, centers, metric=self.metric_mapping[self.metric])\n",
    "    \n",
    "    def _compute_centers(self, X, labels):\n",
    "        \"\"\"Compute new centers using specified method.\"\"\"\n",
    "        new_centers = np.zeros((self.n_clusters, X.shape[1]))\n",
    "        \n",
    "        for i in range(self.n_clusters):\n",
    "            cluster_points = X[labels == i]\n",
    "            \n",
    "            if len(cluster_points) == 0:\n",
    "                continue\n",
    "                \n",
    "            if self.center_method == 'mean':\n",
    "                new_centers[i] = np.mean(cluster_points, axis=0)\n",
    "            \n",
    "            elif self.center_method == 'median':\n",
    "                new_centers[i] = np.median(cluster_points, axis=0)\n",
    "            \n",
    "            elif self.center_method == 'medoid':\n",
    "                # For medoid, find the point that minimizes sum of distances to other points\n",
    "                distances = self._compute_distances(cluster_points, cluster_points)\n",
    "                medoid_idx = np.argmin(np.sum(distances, axis=1))\n",
    "                new_centers[i] = cluster_points[medoid_idx]\n",
    "        \n",
    "        return new_centers\n",
    "    \n",
    "    def _single_fit(self, X, seed):\n",
    "        \"\"\"Perform a single run of k-means with given random seed.\"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        # Initialize centers randomly\n",
    "        idx = np.random.choice(len(X), self.n_clusters, replace=False)\n",
    "        centers = X[idx].copy()\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            # Store old centers for convergence check\n",
    "            old_centers = centers.copy()\n",
    "            \n",
    "            # Assign points to nearest center\n",
    "            distances = self._compute_distances(X, centers)\n",
    "            labels = np.argmin(distances, axis=1)\n",
    "            \n",
    "            # Update centers\n",
    "            centers = self._compute_centers(X, labels)\n",
    "            \n",
    "            # Check for convergence\n",
    "            if np.allclose(old_centers, centers):\n",
    "                n_iter = iteration + 1\n",
    "                break\n",
    "        else:\n",
    "            n_iter = self.max_iter\n",
    "            \n",
    "        # Compute final inertia\n",
    "        final_distances = self._compute_distances(X, centers)\n",
    "        inertia = np.sum(np.min(final_distances, axis=1) ** 2)\n",
    "        \n",
    "        return centers, labels, inertia, n_iter\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit the model to the data.\"\"\"\n",
    "        # Convert pandas DataFrame to numpy array if necessary\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        X = np.asarray(X)\n",
    "        \n",
    "        # Initialize best solution tracking\n",
    "        best_inertia = np.inf\n",
    "        best_labels = None\n",
    "        best_centers = None\n",
    "        best_n_iter = None\n",
    "        \n",
    "        # Run k-means n_init times\n",
    "        for init in range(self.n_init):\n",
    "            # Generate seed for this initialization\n",
    "            if self.random_state is not None:\n",
    "                seed = self.random_state + init\n",
    "            else:\n",
    "                seed = None\n",
    "                \n",
    "            # Perform single k-means run\n",
    "            centers, labels, inertia, n_iter = self._single_fit(X, seed)\n",
    "            \n",
    "            # Update best solution if current one is better\n",
    "            if inertia < best_inertia:\n",
    "                best_centers = centers\n",
    "                best_labels = labels\n",
    "                best_inertia = inertia\n",
    "                best_n_iter = n_iter\n",
    "        \n",
    "        # Store best solution\n",
    "        self.cluster_centers_ = best_centers\n",
    "        self.labels_ = best_labels\n",
    "        self.inertia_ = best_inertia\n",
    "        self.n_iter_ = best_n_iter\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        \"\"\"Fit the model and return cluster labels.\"\"\"\n",
    "        return self.fit(X).labels_\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the closest cluster for each sample in X.\"\"\"\n",
    "        # Convert pandas DataFrame to numpy array if necessary\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        X = np.asarray(X)\n",
    "        \n",
    "        distances = self._compute_distances(X, self.cluster_centers_)\n",
    "        return np.argmin(distances, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c980f3-14fc-4f4b-955a-24cacb5dc06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the models\n",
    "def do_kmeans(n, dist, link):\n",
    "    kmeans = FlexibleKMeans(\n",
    "        n_clusters = n,\n",
    "        metric = dist,\n",
    "        center_method = link,\n",
    "        n_init = 25,\n",
    "        random_state = 43)\n",
    "\n",
    "    pred_clust = kmeans.fit_predict(data)\n",
    "    \n",
    "    model = 'kmeans'\n",
    "    params = f\"distance = {dist}, linkage = {link}\"\n",
    "    \n",
    "    return get_metrics(model, params, n, data, pred_clust)\n",
    "\n",
    "data = data_n_scaled\n",
    "\n",
    "clust_range = range(1, max_clust+1)\n",
    "distances = ['euclidean', 'manhattan', 'chebyshev']\n",
    "linkages = ['mean', 'median', 'medoid']\n",
    "params = product(clust_range, distances, linkages)\n",
    "\n",
    "results = Parallel(n_jobs=max_threads)(delayed(do_kmeans)(n, dist, link) for n, dist, link in params)\n",
    "kmeans_all = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506f5dc4-921c-48c0-9433-5ff49e6b91b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model for each combination of parameters through the Elbow method\n",
    "def elbow_method(dist, link, val_index):\n",
    "    params = f\"distance = {dist}, linkage = {link}\"\n",
    "    res = kmeans_all[kmeans_all['params'] == params]\n",
    "    \n",
    "    res = res.dropna(subset=[val_index])\n",
    "\n",
    "    x = res[\"n_clust\"]\n",
    "    y = res[val_index]\n",
    "\n",
    "    if val_index == 'davies_bouldin':\n",
    "        knee_locator = KneeLocator(x, y, curve='concave', direction='increasing')\n",
    "    else:\n",
    "        knee_locator = KneeLocator(x, y, curve='convex', direction='decreasing')\n",
    "    \n",
    "    return res[res[\"n_clust\"] == knee_locator.knee]\n",
    "\n",
    "kmeans_elbow = pd.DataFrame()\n",
    "\n",
    "distances = ['euclidean', 'manhattan', 'chebyshev']\n",
    "linkages = ['mean', 'median', 'medoid']\n",
    "models = product(distances, linkages)\n",
    "\n",
    "for dist, link in models:\n",
    "    for val_index in val_indexes:\n",
    "        best_mod = elbow_method(dist, link, val_index)\n",
    "        kmeans_elbow = pd.concat([kmeans_elbow, best_mod], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a605a2-5736-415f-ad55-1f358a3b0fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find absolute best models for each validity index\n",
    "kmeans_elbow = kmeans_elbow.drop_duplicates().reset_index(drop=True)\n",
    "# Need to add colums indicating which validity index is maximized.\n",
    "# After that, duplicate models should be merged, not dropped.\n",
    "\n",
    "best_silhouette_euclid = kmeans_elbow.sort_values('silhouette_euclid', ascending=False).iloc[0]\n",
    "best_silhouette_hamm = kmeans_elbow.sort_values('silhouette_hamm', ascending=False).iloc[0]\n",
    "best_ch = kmeans_elbow.sort_values('calinski_harabasz', ascending=False).iloc[0]\n",
    "best_db = kmeans_elbow.sort_values('davies_bouldin', ascending=True).iloc[0]\n",
    "best_dunn = kmeans_elbow.sort_values('dunn', ascending=False).iloc[0]\n",
    "best_inertia = kmeans_elbow.sort_values('inertia', ascending=False).iloc[0]\n",
    "\n",
    "kmeans_best = pd.DataFrame([best_silhouette_euclid, best_silhouette_hamm, best_ch, best_db, best_dunn, best_inertia])\n",
    "kmeans_best = kmeans_best.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761079d6-6a2b-4b72-8aaf-e2942c5312af",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58701c7f-5dd7-44f8-ba19-3d0b3979be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit and plot the best model\n",
    "data = data_n_scaled\n",
    "\n",
    "kmeans = FlexibleKMeans(\n",
    "    n_clusters = 3,\n",
    "    metric = 'manhattan',\n",
    "    center_method = 'mean',\n",
    "    n_init = 25,\n",
    "    random_state = 43)\n",
    "\n",
    "pred_clust = kmeans.fit_predict(data)\n",
    "\n",
    "plot_clusters(data, range(0,3), pred_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1117e279-13f1-4aa3-a824-b203234dcf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_cluster_profiles(data_n, pred_clust, feature_names = None, class_names = None, sd = 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24849e55-57cb-4680-9a5b-c8951503d389",
   "metadata": {},
   "source": [
    "# AHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b6d279-76b0-4cc9-a36c-d1b0db6bdd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the models\n",
    "def do_AHC(n, dist, link):\n",
    "    ahc = AgglomerativeClustering(\n",
    "        n_clusters = n,\n",
    "        metric = dist,\n",
    "        linkage = link)\n",
    "    \n",
    "    ahc.fit(data)\n",
    "    pred_clust = ahc.labels_\n",
    "\n",
    "    model = 'AHC'\n",
    "    params = f\"distance = {dist}, linkage = {link}\"\n",
    "\n",
    "    return get_metrics(model, params, n, data, pred_clust)\n",
    "\n",
    "data = data_n_scaled\n",
    "\n",
    "clust_range = range(1, max_clust+1)\n",
    "distances = ['manhattan', 'euclidean', 'chebyshev']\n",
    "linkages = ['single', 'average', 'complete']\n",
    "params = product(clust_range, distances, linkages)\n",
    "\n",
    "results = Parallel(n_jobs=max_threads)(delayed(do_AHC)(n, dist, link) for n, dist, link in params)\n",
    "results.extend([do_AHC(n, 'euclidean', 'ward') for n in clust_range])\n",
    "ahc_all = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7351b9-0ffc-4a85-b55f-569c986db981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model for each combination of parameters through the Elbow method\n",
    "def elbow_method(dist, link, val_index):\n",
    "    params = f\"distance = {dist}, linkage = {link}\"\n",
    "    res = ahc_all[ahc_all['params'] == params]\n",
    "    \n",
    "    res = res.dropna(subset=[val_index])\n",
    "\n",
    "    x = res[\"n_clust\"]\n",
    "    y = res[val_index]\n",
    "\n",
    "    if val_index == 'davies_bouldin':\n",
    "        knee_locator = KneeLocator(x, y, curve='concave', direction='increasing')\n",
    "    else:\n",
    "        knee_locator = KneeLocator(x, y, curve='convex', direction='decreasing')\n",
    "    \n",
    "    return res[res[\"n_clust\"] == knee_locator.knee]\n",
    "\n",
    "ahc_elbow = pd.DataFrame()\n",
    "\n",
    "distances = ['manhattan', 'euclidean', 'chebyshev']\n",
    "linkages = ['single', 'average', 'complete']\n",
    "models = product(distances, linkages)\n",
    "\n",
    "for dist, link in models:\n",
    "    for val_index in val_indexes:\n",
    "        best_mod = elbow_method(dist, link, val_index)\n",
    "        ahc_elbow = pd.concat([ahc_elbow, best_mod], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8c12d3-f653-4861-a828-285aebb43187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find absolute best models for each validity index\n",
    "ahc_elbow = ahc_elbow.drop_duplicates().reset_index(drop=True)\n",
    "# Need to add colums indicating which validity index is maximized.\n",
    "# After that, duplicate models should be merged, not dropped.\n",
    "\n",
    "best_silhouette_euclid = ahc_elbow.sort_values('silhouette_euclid', ascending=False).iloc[0]\n",
    "best_silhouette_hamm = ahc_elbow.sort_values('silhouette_hamm', ascending=False).iloc[0]\n",
    "best_ch = ahc_elbow.sort_values('calinski_harabasz', ascending=False).iloc[0]\n",
    "best_db = ahc_elbow.sort_values('davies_bouldin', ascending=True).iloc[0]\n",
    "best_dunn = ahc_elbow.sort_values('dunn', ascending=False).iloc[0]\n",
    "best_inertia = ahc_elbow.sort_values('inertia', ascending=False).iloc[0]\n",
    "\n",
    "ahc_best = pd.DataFrame([best_silhouette_euclid, best_silhouette_hamm, best_ch, best_db, best_dunn, best_inertia])\n",
    "ahc_best = ahc_best.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987589e4-0282-450f-a274-c814fec146b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ahc_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f597a400-a862-40ee-b669-d7a65cdc2fdf",
   "metadata": {},
   "source": [
    "AHC yields only one interesting model, where the smallest cluster is not nearly empty. This model have 4 clusters. But its biggest cluster gathers 85 % of the individuals, meaning the others are really small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd6c3c0-05d7-47f7-be3c-9ceca1c98c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit and plot the best model\n",
    "data = data_n_scaled\n",
    "\n",
    "ahc = AgglomerativeClustering(\n",
    "    n_clusters = 4,\n",
    "    metric = 'manhattan',\n",
    "    linkage = 'complete')\n",
    "    \n",
    "ahc.fit(data)\n",
    "pred_clust = ahc.labels_\n",
    "\n",
    "plot_clusters(data, range(0,4), pred_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1510319f-ebd4-4c90-a897-41fb0f153491",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_cluster_profiles(data_n, pred_clust, feature_names = None, class_names = None, sd = 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf5a07e-2a44-4e5e-826d-7a2ac4b6fe57",
   "metadata": {},
   "source": [
    "# HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b99687-9544-4c98-b39e-ca692cf0de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the models\n",
    "def do_hdbscan(dist, min_c, min_s):\n",
    "    if dist == 'mahalanobis':\n",
    "        cov_matrix = np.cov(data, rowvar=False)  # Compute covariance\n",
    "        inv_cov_matrix = np.linalg.inv(cov_matrix)  # Compute inverse\n",
    "\n",
    "        # Define a Mahalanobis distance function\n",
    "        def mahalanobis_metric(a, b):\n",
    "            return mahalanobis(a, b, inv_cov_matrix)\n",
    "\n",
    "        dist_func = mahalanobis_metric\n",
    "    else:\n",
    "        dist_func = dist\n",
    "        \n",
    "    hdb = HDBSCAN(\n",
    "        metric = dist_func,\n",
    "        min_cluster_size = min_c, \n",
    "        min_samples = min_s)\n",
    "        \n",
    "    pred_clust = hdb.fit_predict(data)\n",
    "\n",
    "    model = 'HDBSCAN'\n",
    "    params = f\"distance = {dist}, min_cluster_size = {min_c}, min_samples = {min_s}\"\n",
    "    n = len(set(pred_clust[pred_clust != -1]))\n",
    "    noise_freq = 100 * sum(pred_clust == -1) / len(pred_clust)\n",
    "\n",
    "    return get_metrics(model, params, n, data, pred_clust, noise = noise_freq)\n",
    "\n",
    "data = data_n_scaled\n",
    "\n",
    "distances = ['manhattan', 'euclidean', 'chebyshev', 'mahalanobis']\n",
    "min_cluster_sizes = range(2, 21)\n",
    "min_samples_range = range(1, 21)\n",
    "params = product(distances, min_cluster_sizes, min_samples_range)\n",
    "\n",
    "results = Parallel(n_jobs=max_threads)(delayed(do_hdbscan)(dist, min_c, min_s) for dist, min_c, min_s in params)\n",
    "hdbscan_all = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968b43fd-3261-4b35-885b-679ae6215117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Elbow method is inapplicable here. We simply select the model maximizing each validity index.\n",
    "best_silhouette_euclid = hdbscan_all.sort_values('silhouette_euclid', ascending=False).iloc[0]\n",
    "best_silhouette_hamm = hdbscan_all.sort_values('silhouette_hamm', ascending=False).iloc[0]\n",
    "best_ch = hdbscan_all.sort_values('calinski_harabasz', ascending=False).iloc[0]\n",
    "best_db = hdbscan_all.sort_values('davies_bouldin', ascending=True).iloc[0]\n",
    "best_dunn = hdbscan_all.sort_values('dunn', ascending=False).iloc[0]\n",
    "best_inertia = hdbscan_all.sort_values('inertia', ascending=False).iloc[0]\n",
    "\n",
    "hdbscan_best = pd.DataFrame([best_silhouette_euclid, best_silhouette_hamm, best_ch, best_db, best_dunn, best_inertia])\n",
    "hdbscan_best = hdbscan_best.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8041b4-7446-4d90-a7c3-a3df81cb53f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a32c9a-09b3-4d8a-9b71-152d93c37c66",
   "metadata": {},
   "source": [
    "HDBSCAN clusters all the individuals together. Based on density, there is only one cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a21c7c9-0aa6-4edd-85ab-a71147062c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of the number of clusters selected by HDBSCAN models, grouping the values above 8\n",
    "bins = list(range(0, 9)) + [8.5]\n",
    "labels = list(range(0, 8)) + ['8+']\n",
    "plot_data = hdbscan_all['n_clust'].apply(lambda x: x if x <= 8 else 8.5)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(plot_data, bins=bins, edgecolor='black', align='left', rwidth=0.8)\n",
    "plt.xticks(bins[:-1], labels)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Number of Models')\n",
    "plt.title('Number of clusters selected by HDBSCAN models')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd2cdbf-2bd5-4a95-960e-1ad75ee38245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit and plot the best model\n",
    "data = data_n_scaled\n",
    "\n",
    "hdb = HDBSCAN(\n",
    "    metric = 'euclidean', \n",
    "    min_cluster_size = 4, \n",
    "    min_samples = 2)\n",
    "\n",
    "pred_clust = hdb.fit_predict(data)\n",
    "n_clusters = len(set(pred_clust[pred_clust != -1]))\n",
    "\n",
    "plot_clusters(data, range(0,n_clusters), pred_clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697f076e-6827-4bb5-8398-80c9a5151938",
   "metadata": {},
   "source": [
    "# Aggregate and display results\n",
    "\n",
    "Not sure it makes sense to compare models that work so differently. Seems better to analyze their results separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44f939c-246a-4741-a7c1-810f1aaf35bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mod_list = [kmeans_best, ahc_best, hdbscan_best]\n",
    "best_models = pd.concat(best_mod_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5961a1-7104-4748-8390-554a430ca628",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb391ae1-da07-4c1e-bc6f-47bea22d93e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the best performing model on each criteria across model classes eliminates hdbscan models\n",
    "# Which could mean hdbscan is underperforming\n",
    "# Or is picking non-convex clusters\n",
    "# Or that data is non-clusterable!\n",
    "best_mod_list = [kmeans_best, ahc_best, hdbscan_best]\n",
    "best_models = pd.concat(best_mod_list, ignore_index=True)\n",
    "\n",
    "best_silhouette_euclid = best_models.sort_values('silhouette_euclid', ascending=False).iloc[0]\n",
    "best_silhouette_hamm = best_models.sort_values('silhouette_hamm', ascending=False).iloc[0]\n",
    "best_ch = best_models.sort_values('calinski_harabasz', ascending=False).iloc[0]\n",
    "best_db = best_models.sort_values('davies_bouldin', ascending=True).iloc[0]\n",
    "best_dunn = best_models.sort_values('dunn', ascending=False).iloc[0]\n",
    "best_inertia = best_models.sort_values('inertia', ascending=False).iloc[0]\n",
    "\n",
    "best_models = pd.DataFrame([best_silhouette_euclid, best_silhouette_hamm, best_ch, best_db, best_dunn, best_inertia])\n",
    "best_models = best_models.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42faee4a-4cee-41d1-8bcd-3ab5a699f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a79babe-8f25-491a-a288-df2da0cff964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "bins = np.arange(best_models['n_clust'].min() - 0.5, best_models['n_clust'].max() + 1.5, 1)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(best_models['n_clust'], bins=bins, edgecolor='black', rwidth=0.8)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Number of Models')\n",
    "plt.title('Optimal number of clusters according to best models')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91d0f2f5-49b2-4e43-908f-142e8f3ef340",
   "metadata": {},
   "source": [
    "# Hopkins Statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19619b2d-75a6-4033-ac85-8edbcf35c8de",
   "metadata": {},
   "source": [
    "Function from the pyclustertend package, which could not be installed because its depencies are outdated.\n",
    "See: https://pyclustertend.readthedocs.io/en/latest/_modules/pyclustertend/hopkins.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabe45e4-854e-4523-baa7-f6e03b471f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hopkins(data_frame, sampling_size):\n",
    "    \"\"\"Assess the clusterability of a dataset. A score between 0 and 1, a score around 0.5 express\n",
    "    no clusterability and a score tending to 0 express a high cluster tendency.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_frame : numpy array\n",
    "        The input dataset\n",
    "    sampling_size : int\n",
    "        The sampling size which is used to evaluate the number of DataFrame.\n",
    "\n",
    "    Returns\n",
    "    ---------------------\n",
    "    score : float\n",
    "        The hopkins score of the dataset (between 0 and 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(data_frame) == np.ndarray:\n",
    "        data_frame = pd.DataFrame(data_frame)\n",
    "\n",
    "    # Sample n observations from D:P\n",
    "    if sampling_size > data_frame.shape[0]:\n",
    "        raise Exception(\n",
    "            'The number of sample of sample is bigger than the shape of D')\n",
    "\n",
    "    data_frame_sample = data_frame.sample(n=sampling_size)\n",
    "\n",
    "    # Get the distance to their neirest neighbors in D:X\n",
    "    tree = BallTree(data_frame, leaf_size=2)\n",
    "    dist, _ = tree.query(data_frame_sample, k=2)\n",
    "    data_frame_sample_distances_to_nearest_neighbours = dist[:, 1]\n",
    "\n",
    "    # Randomly simulate n points with the same variation as in D:Q\n",
    "    max_data_frame = data_frame.max()\n",
    "    min_data_frame = data_frame.min()\n",
    "\n",
    "    uniformly_selected_values_0 = np.random.uniform(min_data_frame[0], max_data_frame[0], sampling_size)\n",
    "    uniformly_selected_values_1 = np.random.uniform(min_data_frame[1], max_data_frame[1], sampling_size)\n",
    "\n",
    "    uniformly_selected_observations = np.column_stack((uniformly_selected_values_0, uniformly_selected_values_1))\n",
    "    if len(max_data_frame) >= 2:\n",
    "        for i in range(2, len(max_data_frame)):\n",
    "            uniformly_selected_values_i = np.random.uniform(min_data_frame[i], max_data_frame[i], sampling_size)\n",
    "            to_stack = (uniformly_selected_observations, uniformly_selected_values_i)\n",
    "            uniformly_selected_observations = np.column_stack(to_stack)\n",
    "\n",
    "    uniformly_selected_observations_df = pd.DataFrame(uniformly_selected_observations)\n",
    "\n",
    "    # Get the distance to their neirest neighbors in D:Y\n",
    "    tree = BallTree(data_frame, leaf_size=2)\n",
    "    dist, _ = tree.query(uniformly_selected_observations_df, k=1)\n",
    "    uniformly_df_distances_to_nearest_neighbours = dist\n",
    "\n",
    "    # Return the hopkins score\n",
    "    x = sum(data_frame_sample_distances_to_nearest_neighbours)\n",
    "    y = sum(uniformly_df_distances_to_nearest_neighbours)\n",
    "\n",
    "    if x + y == 0:\n",
    "        raise Exception('The denominator of the hopkins statistics is null')\n",
    "\n",
    "    return x / (x + y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fc5cdf-91b2-48ed-bef2-0a9c0fcfc470",
   "metadata": {},
   "outputs": [],
   "source": [
    "float(hopkins(data_n.values, data_n.shape[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
