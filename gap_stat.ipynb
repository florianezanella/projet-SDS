{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed18269-9962-41f2-beb3-45893a3ad8f9",
   "metadata": {},
   "source": [
    "Experimental implementation of the gap statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4887e505-68fb-40c0-adb5-06ef3199e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "from joblib import Parallel, delayed # for parallelization\n",
    "from itertools import product\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Clustering\n",
    "from stepmix.stepmix import StepMix\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import AgglomerativeClustering, HDBSCAN\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.neighbors import BallTree\n",
    "import torch\n",
    "from torchmetrics.clustering import DunnIndex\n",
    "from collections import Counter\n",
    "from kneed import KneeLocator\n",
    "\n",
    "# Visualization\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import ConvexHull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d1e61c-3574-4482-9529-59268b7fb8d6",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1190dd79-e4b5-4684-98b0-0f6ce7797686",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2004_i = pd.read_parquet(\"data/data2004_i.parquet\") # load imputed data\n",
    "\n",
    "# Dataset with numeric outcomes\n",
    "data_n = data2004_i[[\n",
    "    # Q2\n",
    "    'clseusa_n', # 'clsetown_n', 'clsestat_n', 'clsenoam_n',\n",
    "    # Q3\n",
    "    'ambornin_n', 'amcit_n', 'amlived_n', 'amenglsh_n', \n",
    "    'amchrstn_n', 'amgovt_n', 'amfeel_n', # 'amancstr_n',\n",
    "    # Q4\n",
    "    'amcitizn_n', 'amshamed_n', 'belikeus_n', 'ambetter_n', 'ifwrong_n', # 'amsports_n', 'lessprd_n',\n",
    "    # Q5\n",
    "    'proudsss_n', 'proudgrp_n', 'proudpol_n', 'prouddem_n', 'proudeco_n',\n",
    "    'proudspt_n', 'proudart_n', 'proudhis_n', 'proudmil_n', 'proudsci_n']]\n",
    "\n",
    "# Dataset with categorical outcomes\n",
    "data_f = data2004_i[[\n",
    "    # Q2\n",
    "    'clseusa_f', # 'clsetown_f', 'clsestat_f', 'clsenoam_f',\n",
    "    # Q3\n",
    "    'ambornin_f', 'amcit_f', 'amlived_f', 'amenglsh_f', \n",
    "    'amchrstn_f', 'amgovt_f', 'amfeel_f', # 'amancstr_f',\n",
    "    # Q4\n",
    "    'amcitizn_f', 'amshamed_f', 'belikeus_f', 'ambetter_f', 'ifwrong_f', # 'amsports_f', 'lessprd_f',\n",
    "    # Q5\n",
    "    'proudsss_f', 'proudgrp_f', 'proudpol_f', 'prouddem_f', 'proudeco_f',\n",
    "    'proudspt_f', 'proudart_f', 'proudhis_f', 'proudmil_f', 'proudsci_f']]\n",
    "\n",
    "# Dataset with controls\n",
    "controls = data2004_i[[\n",
    "    'sex', 'race_f', 'born_usa', 'party_fs', 'religstr_f', \n",
    "    'reltrad_f', 'region_f']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b97bf-43e0-4516-b3ee-f226e4ebd772",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c55b24ac-f239-4dea-9283-e3491ec65ee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_clust = 12\n",
    "max_threads = 8\n",
    "\n",
    "val_indexes = ['silhouette', 'calinski_harabasz', 'davies_bouldin', 'dunn', 'inertia']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d170e0b-2c98-40ec-bda1-12743bfd5b9e",
   "metadata": {},
   "source": [
    "## Validity indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01c84df2-d928-437c-b4f8-200cc8b0d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom score functions to avoid throwing errors when undefined\n",
    "def sil_score(data, pred_clust):\n",
    "    try:\n",
    "        sil_score = silhouette_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        sil_score = np.nan\n",
    "    return sil_score\n",
    "\n",
    "def ch_score(data, pred_clust):\n",
    "    try:\n",
    "        ch_score = calinski_harabasz_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        ch_score = np.nan\n",
    "    return ch_score\n",
    "\n",
    "def db_score(data, pred_clust):\n",
    "    try:\n",
    "        db_score = davies_bouldin_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        db_score = np.nan\n",
    "    return db_score\n",
    "\n",
    "def dunn_score(data, pred_clust):\n",
    "    torch_data = np.array(data)\n",
    "    torch_data = torch.tensor(torch_data, dtype=torch.float32)\n",
    "    torch_pred_clust = torch.tensor(pred_clust, dtype=torch.int64)\n",
    "\n",
    "    dunn_metric = DunnIndex()\n",
    "    \n",
    "    try:\n",
    "        dunn_score = float(dunn_metric(torch_data, torch_pred_clust))\n",
    "    except Exception:\n",
    "        dunn_score = np.nan\n",
    " \n",
    "    return dunn_score\n",
    "\n",
    "def inertia(data, pred_clust):\n",
    "    data = np.asarray(data)\n",
    "    \n",
    "    inertia = 0\n",
    "    for cluster in np.unique(pred_clust):\n",
    "        cluster_points = data[pred_clust == cluster]\n",
    "        cluster_centroid = np.mean(cluster_points, axis=0)\n",
    "        inertia += np.sum((cluster_points - cluster_centroid) ** 2)\n",
    "        \n",
    "    return inertia\n",
    "\n",
    "def clust_size(pred_clust):\n",
    "    cluster_sizes = Counter(pred_clust)\n",
    "    min_size = min(cluster_sizes.values())\n",
    "    max_size = max(cluster_sizes.values())\n",
    "    \n",
    "    return min_size, max_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf65c0e6-238a-4879-87e3-074f85d8333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return all validity indexes at once\n",
    "def get_metrics(model, params, n, data, pred_clust, **additional_metrics):\n",
    "    base_metrics = {\n",
    "        'model': model,\n",
    "        'params': params,\n",
    "        'n_clust': n,\n",
    "        'min_clust_size': clust_size(pred_clust)[0],\n",
    "        'max_clust_size': clust_size(pred_clust)[1],\n",
    "        'silhouette': float(sil_score(data, pred_clust)),\n",
    "        'calinski_harabasz': float(ch_score(data, pred_clust)),\n",
    "        'davies_bouldin': float(db_score(data, pred_clust)),\n",
    "        'dunn': float(dunn_score(data, pred_clust)),\n",
    "        'inertia': float(inertia(data, pred_clust))\n",
    "    }\n",
    "\n",
    "    base_metrics.update(additional_metrics)\n",
    "    return base_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1513b5bc-963b-41e5-a6bd-12f45f22c2f5",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c78aabed-4cb1-4e87-a35f-797240d34b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "clust_range = range(1, max_clust+1)\n",
    "\n",
    "opt_params = {\n",
    "    'method': 'gradient',\n",
    "    'intercept': True,\n",
    "    'max_iter': 2500,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa743e87-3ef4-4e07-9610-59a15540bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit models without covariates\n",
    "def do_StepMix(n, type, data):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "        latent_mod = StepMix(\n",
    "            n_components = n, \n",
    "            measurement = type, \n",
    "            n_init = 3,\n",
    "            init_params = 'kmeans',\n",
    "            structural_params = opt_params,\n",
    "            random_state = 123,\n",
    "            progress_bar = 0)\n",
    "        \n",
    "        latent_mod.fit(data)\n",
    "        pred_clust = latent_mod.predict(data)\n",
    "\n",
    "        model = 'LCA' if type == 'categorical' else 'LPA'\n",
    "        params = 'without covariates'\n",
    "        loglik = latent_mod.score(data)\n",
    "        aic = latent_mod.aic(data)\n",
    "        bic = latent_mod.aic(data)\n",
    "        entropy = latent_mod.entropy(data)\n",
    "\n",
    "    return get_metrics(model, params, n, data, pred_clust, LL = loglik, aic = aic, bic = bic, entropy = entropy)\n",
    "\n",
    "data = data_f.apply(lambda col: LabelEncoder().fit_transform(col))\n",
    "cat_results = Parallel(n_jobs=8)(delayed(do_StepMix)(n, 'categorical', data) for n in clust_range)\n",
    "LCA_all = pd.DataFrame(cat_results)\n",
    "\n",
    "num_results = Parallel(n_jobs=8)(delayed(do_StepMix)(n, 'continuous', data_n) for n in clust_range)\n",
    "LPA_all = pd.DataFrame(num_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9748fe56-0c8f-4493-be53-969f3b3ccf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleKMeans:\n",
    "    \"\"\"\n",
    "    K-Means implementation supporting different distance metrics and center computation methods.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_clusters : int\n",
    "        Number of clusters\n",
    "    metric : str, default='euclidean'\n",
    "        Distance metric: 'euclidean', 'manhattan', 'chebyshev'\n",
    "    center_method : str, default='mean'\n",
    "        Method to compute cluster centers: 'mean', 'median', 'medoid'\n",
    "    max_iter : int, default=100\n",
    "        Maximum number of iterations\n",
    "    n_init : int, default=10\n",
    "        Number of times the k-means algorithm will be run with different centroid seeds.\n",
    "        The final result will be the best output of n_init consecutive runs in terms of inertia.\n",
    "    random_state : int or None, default=None\n",
    "        Random state for reproducibility\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_clusters, metric='euclidean', center_method='mean', \n",
    "                 max_iter=100, n_init=10, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.metric = metric\n",
    "        self.center_method = center_method\n",
    "        self.max_iter = max_iter\n",
    "        self.n_init = n_init\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Define mapping from user-friendly names to scipy metrics\n",
    "        self.metric_mapping = {\n",
    "            'euclidean': 'euclidean',\n",
    "            'manhattan': 'cityblock',\n",
    "            'chebyshev': 'chebyshev'\n",
    "        }\n",
    "        \n",
    "        # Validate inputs\n",
    "        valid_metrics = list(self.metric_mapping.keys())\n",
    "        if metric not in valid_metrics:\n",
    "            raise ValueError(f\"metric must be one of {valid_metrics}\")\n",
    "            \n",
    "        valid_centers = ['mean', 'median', 'medoid']\n",
    "        if center_method not in valid_centers:\n",
    "            raise ValueError(f\"center_method must be one of {valid_centers}\")\n",
    "            \n",
    "        if self.n_init <= 0:\n",
    "            raise ValueError(\"n_init should be > 0\")\n",
    "    \n",
    "    def _compute_distances(self, X, centers):\n",
    "        \"\"\"Compute distances between points and centers using specified metric.\"\"\"\n",
    "        return cdist(X, centers, metric=self.metric_mapping[self.metric])\n",
    "    \n",
    "    def _compute_centers(self, X, labels):\n",
    "        \"\"\"Compute new centers using specified method.\"\"\"\n",
    "        new_centers = np.zeros((self.n_clusters, X.shape[1]))\n",
    "        \n",
    "        for i in range(self.n_clusters):\n",
    "            cluster_points = X[labels == i]\n",
    "            \n",
    "            if len(cluster_points) == 0:\n",
    "                continue\n",
    "                \n",
    "            if self.center_method == 'mean':\n",
    "                new_centers[i] = np.mean(cluster_points, axis=0)\n",
    "            \n",
    "            elif self.center_method == 'median':\n",
    "                new_centers[i] = np.median(cluster_points, axis=0)\n",
    "            \n",
    "            elif self.center_method == 'medoid':\n",
    "                # For medoid, find the point that minimizes sum of distances to other points\n",
    "                distances = self._compute_distances(cluster_points, cluster_points)\n",
    "                medoid_idx = np.argmin(np.sum(distances, axis=1))\n",
    "                new_centers[i] = cluster_points[medoid_idx]\n",
    "        \n",
    "        return new_centers\n",
    "    \n",
    "    def _single_fit(self, X, seed):\n",
    "        \"\"\"Perform a single run of k-means with given random seed.\"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        # Initialize centers randomly\n",
    "        idx = np.random.choice(len(X), self.n_clusters, replace=False)\n",
    "        centers = X[idx].copy()\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            # Store old centers for convergence check\n",
    "            old_centers = centers.copy()\n",
    "            \n",
    "            # Assign points to nearest center\n",
    "            distances = self._compute_distances(X, centers)\n",
    "            labels = np.argmin(distances, axis=1)\n",
    "            \n",
    "            # Update centers\n",
    "            centers = self._compute_centers(X, labels)\n",
    "            \n",
    "            # Check for convergence\n",
    "            if np.allclose(old_centers, centers):\n",
    "                n_iter = iteration + 1\n",
    "                break\n",
    "        else:\n",
    "            n_iter = self.max_iter\n",
    "            \n",
    "        # Compute final inertia\n",
    "        final_distances = self._compute_distances(X, centers)\n",
    "        inertia = np.sum(np.min(final_distances, axis=1) ** 2)\n",
    "        \n",
    "        return centers, labels, inertia, n_iter\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit the model to the data.\"\"\"\n",
    "        # Convert pandas DataFrame to numpy array if necessary\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        X = np.asarray(X)\n",
    "        \n",
    "        # Initialize best solution tracking\n",
    "        best_inertia = np.inf\n",
    "        best_labels = None\n",
    "        best_centers = None\n",
    "        best_n_iter = None\n",
    "        \n",
    "        # Run k-means n_init times\n",
    "        for init in range(self.n_init):\n",
    "            # Generate seed for this initialization\n",
    "            if self.random_state is not None:\n",
    "                seed = self.random_state + init\n",
    "            else:\n",
    "                seed = None\n",
    "                \n",
    "            # Perform single k-means run\n",
    "            centers, labels, inertia, n_iter = self._single_fit(X, seed)\n",
    "            \n",
    "            # Update best solution if current one is better\n",
    "            if inertia < best_inertia:\n",
    "                best_centers = centers\n",
    "                best_labels = labels\n",
    "                best_inertia = inertia\n",
    "                best_n_iter = n_iter\n",
    "        \n",
    "        # Store best solution\n",
    "        self.cluster_centers_ = best_centers\n",
    "        self.labels_ = best_labels\n",
    "        self.inertia_ = best_inertia\n",
    "        self.n_iter_ = best_n_iter\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        \"\"\"Fit the model and return cluster labels.\"\"\"\n",
    "        return self.fit(X).labels_\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the closest cluster for each sample in X.\"\"\"\n",
    "        # Convert pandas DataFrame to numpy array if necessary\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        X = np.asarray(X)\n",
    "        \n",
    "        distances = self._compute_distances(X, self.cluster_centers_)\n",
    "        return np.argmin(distances, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95c7be18-a915-4bc1-ae86-ae1f39cc1084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_kmeans(data, n, dist, link):\n",
    "    kmeans = FlexibleKMeans(\n",
    "        n_clusters = n,\n",
    "        metric = dist,\n",
    "        center_method = link,\n",
    "        n_init = 15)\n",
    "\n",
    "    pred_clust = kmeans.fit_predict(data)\n",
    "    \n",
    "    model = 'kmeans'\n",
    "    params = f\"dist = {dist}, link = {link}\"\n",
    "    \n",
    "    return get_metrics(model, params, n, data, pred_clust)\n",
    "\n",
    "clust_range = range(1, max_clust+1)\n",
    "distances = ['euclidean', 'manhattan', 'chebyshev']\n",
    "linkages = ['mean', 'median', 'medoid']\n",
    "params = product(clust_range, distances, linkages)\n",
    "\n",
    "results = Parallel(n_jobs=max_threads)(delayed(do_kmeans)(data_n, n, dist, link) for n, dist, link in params)\n",
    "kmeans_all = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5915d91a-a1dd-4254-a7cb-3703fedb45a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_AHC(data, n, dist, link):\n",
    "    ahc = AgglomerativeClustering(\n",
    "        n_clusters = n,\n",
    "        metric = dist,\n",
    "        linkage = link)\n",
    "    \n",
    "    ahc.fit(data)\n",
    "    pred_clust = ahc.labels_\n",
    "\n",
    "    model = 'AHC'\n",
    "    params = f\"dist = {dist}, link = {link}\"\n",
    "\n",
    "    return get_metrics(model, params, n, data, pred_clust)\n",
    "\n",
    "clust_range = range(1, max_clust+1)\n",
    "distances = ['manhattan', 'euclidean', 'chebyshev', 'hamming']\n",
    "linkages = ['single', 'average', 'complete']\n",
    "params = product(clust_range, distances, linkages)\n",
    "\n",
    "results = Parallel(n_jobs=max_threads)(delayed(do_AHC)(data_n, n, dist, link) for n, dist, link in params)\n",
    "results.extend([do_AHC(data_n, n, 'euclidean', 'ward') for n in clust_range])\n",
    "ahc_all = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ce063cf-1433-4461-851a-646030af0069",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = pd.concat([LPA_all, kmeans_all, ahc_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "786c50d9-373a-413b-85c7-e62abf303d95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>n_clust</th>\n",
       "      <th>min_clust_size</th>\n",
       "      <th>max_clust_size</th>\n",
       "      <th>silhouette</th>\n",
       "      <th>calinski_harabasz</th>\n",
       "      <th>davies_bouldin</th>\n",
       "      <th>dunn</th>\n",
       "      <th>inertia</th>\n",
       "      <th>LL</th>\n",
       "      <th>aic</th>\n",
       "      <th>bic</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LPA</td>\n",
       "      <td>without covariates</td>\n",
       "      <td>1</td>\n",
       "      <td>1215</td>\n",
       "      <td>1215</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29639.955556</td>\n",
       "      <td>-32.228055</td>\n",
       "      <td>78406.174399</td>\n",
       "      <td>78406.174399</td>\n",
       "      <td>1.214029e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LPA</td>\n",
       "      <td>without covariates</td>\n",
       "      <td>2</td>\n",
       "      <td>384</td>\n",
       "      <td>831</td>\n",
       "      <td>0.150056</td>\n",
       "      <td>145.528168</td>\n",
       "      <td>2.717648</td>\n",
       "      <td>0.334725</td>\n",
       "      <td>26464.866125</td>\n",
       "      <td>-21.416884</td>\n",
       "      <td>52229.028069</td>\n",
       "      <td>52229.028069</td>\n",
       "      <td>1.311807e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LPA</td>\n",
       "      <td>without covariates</td>\n",
       "      <td>3</td>\n",
       "      <td>239</td>\n",
       "      <td>558</td>\n",
       "      <td>0.069649</td>\n",
       "      <td>106.252173</td>\n",
       "      <td>3.243625</td>\n",
       "      <td>0.249888</td>\n",
       "      <td>25218.333831</td>\n",
       "      <td>-18.615251</td>\n",
       "      <td>45515.059593</td>\n",
       "      <td>45515.059593</td>\n",
       "      <td>4.326579e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LPA</td>\n",
       "      <td>without covariates</td>\n",
       "      <td>4</td>\n",
       "      <td>155</td>\n",
       "      <td>568</td>\n",
       "      <td>0.056171</td>\n",
       "      <td>87.379769</td>\n",
       "      <td>4.007530</td>\n",
       "      <td>0.186098</td>\n",
       "      <td>24365.642826</td>\n",
       "      <td>-16.127673</td>\n",
       "      <td>39564.246133</td>\n",
       "      <td>39564.246133</td>\n",
       "      <td>3.451061e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LPA</td>\n",
       "      <td>without covariates</td>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>484</td>\n",
       "      <td>0.032484</td>\n",
       "      <td>73.545596</td>\n",
       "      <td>4.533056</td>\n",
       "      <td>0.138979</td>\n",
       "      <td>23843.083535</td>\n",
       "      <td>-9.411121</td>\n",
       "      <td>23337.025048</td>\n",
       "      <td>23337.025048</td>\n",
       "      <td>2.059169e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>AHC</td>\n",
       "      <td>dist = euclidean, link = ward</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>309</td>\n",
       "      <td>0.023439</td>\n",
       "      <td>84.992333</td>\n",
       "      <td>2.700936</td>\n",
       "      <td>0.244480</td>\n",
       "      <td>19853.769106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>AHC</td>\n",
       "      <td>dist = euclidean, link = ward</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>309</td>\n",
       "      <td>0.014033</td>\n",
       "      <td>78.482504</td>\n",
       "      <td>2.582330</td>\n",
       "      <td>0.253340</td>\n",
       "      <td>19492.101760</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>AHC</td>\n",
       "      <td>dist = euclidean, link = ward</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>263</td>\n",
       "      <td>0.020139</td>\n",
       "      <td>73.450336</td>\n",
       "      <td>2.564881</td>\n",
       "      <td>0.253340</td>\n",
       "      <td>19139.941924</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>AHC</td>\n",
       "      <td>dist = euclidean, link = ward</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>263</td>\n",
       "      <td>0.021917</td>\n",
       "      <td>68.968191</td>\n",
       "      <td>2.489399</td>\n",
       "      <td>0.266087</td>\n",
       "      <td>18845.037460</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>AHC</td>\n",
       "      <td>dist = euclidean, link = ward</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>197</td>\n",
       "      <td>0.026700</td>\n",
       "      <td>65.245637</td>\n",
       "      <td>2.654152</td>\n",
       "      <td>0.254320</td>\n",
       "      <td>18564.496908</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>276 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    model                         params  n_clust  min_clust_size  \\\n",
       "0     LPA             without covariates        1            1215   \n",
       "1     LPA             without covariates        2             384   \n",
       "2     LPA             without covariates        3             239   \n",
       "3     LPA             without covariates        4             155   \n",
       "4     LPA             without covariates        5             155   \n",
       "..    ...                            ...      ...             ...   \n",
       "151   AHC  dist = euclidean, link = ward        8              20   \n",
       "152   AHC  dist = euclidean, link = ward        9              20   \n",
       "153   AHC  dist = euclidean, link = ward       10              20   \n",
       "154   AHC  dist = euclidean, link = ward       11              20   \n",
       "155   AHC  dist = euclidean, link = ward       12              20   \n",
       "\n",
       "     max_clust_size  silhouette  calinski_harabasz  davies_bouldin      dunn  \\\n",
       "0              1215         NaN                NaN             NaN       NaN   \n",
       "1               831    0.150056         145.528168        2.717648  0.334725   \n",
       "2               558    0.069649         106.252173        3.243625  0.249888   \n",
       "3               568    0.056171          87.379769        4.007530  0.186098   \n",
       "4               484    0.032484          73.545596        4.533056  0.138979   \n",
       "..              ...         ...                ...             ...       ...   \n",
       "151             309    0.023439          84.992333        2.700936  0.244480   \n",
       "152             309    0.014033          78.482504        2.582330  0.253340   \n",
       "153             263    0.020139          73.450336        2.564881  0.253340   \n",
       "154             263    0.021917          68.968191        2.489399  0.266087   \n",
       "155             197    0.026700          65.245637        2.654152  0.254320   \n",
       "\n",
       "          inertia         LL           aic           bic       entropy  \n",
       "0    29639.955556 -32.228055  78406.174399  78406.174399  1.214029e-12  \n",
       "1    26464.866125 -21.416884  52229.028069  52229.028069  1.311807e+01  \n",
       "2    25218.333831 -18.615251  45515.059593  45515.059593  4.326579e+01  \n",
       "3    24365.642826 -16.127673  39564.246133  39564.246133  3.451061e+01  \n",
       "4    23843.083535  -9.411121  23337.025048  23337.025048  2.059169e+01  \n",
       "..            ...        ...           ...           ...           ...  \n",
       "151  19853.769106        NaN           NaN           NaN           NaN  \n",
       "152  19492.101760        NaN           NaN           NaN           NaN  \n",
       "153  19139.941924        NaN           NaN           NaN           NaN  \n",
       "154  18845.037460        NaN           NaN           NaN           NaN  \n",
       "155  18564.496908        NaN           NaN           NaN           NaN  \n",
       "\n",
       "[276 rows x 14 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d8e1dc-5d1b-415a-b51f-a7b75be5ca46",
   "metadata": {},
   "source": [
    "# Gap stat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c478565-f20e-412d-b776-b880d8261cfa",
   "metadata": {},
   "source": [
    "Adapted from: https://www.geeksforgeeks.org/gap-statistics-for-optimal-number-of-cluster/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd706014-394e-46fb-bec1-4d42168e536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate reference data from a uniform distribution\n",
    "def gen_ref_data(data):\n",
    "    return np.random.uniform(low=data.min(axis=0), \n",
    "                            high=data.max(axis=0), \n",
    "                            size=data.shape)\n",
    "\n",
    "\n",
    "# Retrieve model parameters for refit\n",
    "def extract_params(str):\n",
    "    pairs = [pair.strip() for pair in str.split(',')]\n",
    "    params = {}\n",
    "    for pair in pairs:\n",
    "        key, value = [x.strip() for x in pair.split('=')]\n",
    "        params[key] = value  \n",
    "    return params\n",
    "\n",
    "\n",
    "# Create empty df to store results\n",
    "def create_empty_df(indices):\n",
    "    cols = ['model', 'params', 'n_clust'] + \\\n",
    "       [f'{idx}_gs' for idx in val_indexes] + \\\n",
    "       [f'{idx}_s' for idx in val_indexes]\n",
    "    float_cols = [col for col in cols if col not in ['model', 'params', 'n_clust']]\n",
    "\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    df[float_cols] = df[float_cols].astype('float64')\n",
    "    df['model'] = df['model'].astype('object')\n",
    "    df['params'] = df['params'].astype('object')\n",
    "    df['n_clust'] = df['n_clust'].astype('int64')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Compute the Gap Statistic\n",
    "def compute_gap_statistic(data, n_min, n_max, iters, model, params):\n",
    "    gap_values = create_empty_df(val_indexes)\n",
    "    \n",
    "    # Loop over n values\n",
    "    for n in range(n_min, n_max + 1):\n",
    "    \n",
    "        # Fit the model on random datasets\n",
    "        rand_scores_all = pd.DataFrame()\n",
    "        \n",
    "        for _ in range(iters):\n",
    "            rand_data = gen_ref_data(data)\n",
    "            \n",
    "            if model == 'LPA' and params == 'without covariates':\n",
    "                rand_scores = do_StepMix(n, 'continuous', rand_data)\n",
    "\n",
    "            elif model == 'kmeans':\n",
    "                config = extract_params(params)\n",
    "                rand_scores = do_kmeans(rand_data, n, **config)\n",
    "\n",
    "            elif model == 'AHC':\n",
    "                config = extract_params(params)\n",
    "                rand_scores = do_AHC(rand_data, n, **config)\n",
    "            \n",
    "            rand_scores = pd.DataFrame([rand_scores])\n",
    "            rand_scores_all = pd.concat([rand_scores_all, rand_scores], ignore_index=True)\n",
    "\n",
    "        # Retrive scores for the assessed model\n",
    "        mod_scores = all_models.loc[(all_models['model'] == model) & \n",
    "                                     (all_models['params'] == params) & \n",
    "                                     (all_models['n_clust'] == n)]\n",
    "\n",
    "        # Calculate the Gap statistic and s value for each validity index\n",
    "        for index in val_indexes:\n",
    "            rand_ind = rand_scores_all[index]\n",
    "            mod_ind = mod_scores[index]\n",
    "\n",
    "            # Rescale the Silhouette index on [0,1] to avoid errors when it is negative\n",
    "            if index == 'silhouette':\n",
    "                rand_ind = (rand_ind + 1) / 2\n",
    "                mod_ind = (mod_ind + 1) / 2\n",
    "                \n",
    "            gap = np.log(np.mean(rand_ind)) - np.log(mod_ind)\n",
    "            s = np.std(np.log(rand_ind)) * np.sqrt(1 + (1 / iters))\n",
    "\n",
    "            # Store the results\n",
    "            ## Check if the corresponding row exists in the df\n",
    "            row_id = ((gap_values['model'] == model) & \n",
    "                      (gap_values['params'] == params) & \n",
    "                      (gap_values['n_clust'] == n))\n",
    "\n",
    "            if gap_values[row_id].empty:\n",
    "            # If not, create a new one\n",
    "                new_row = {\n",
    "                    'model': model,\n",
    "                    'params': params,\n",
    "                    'n_clust': n,\n",
    "                    f'{index}_gs': gap.values[0],\n",
    "                    f'{index}_s': s\n",
    "                }\n",
    "                new_row = pd.DataFrame([new_row])\n",
    "                gap_values = pd.concat([gap_values, new_row], ignore_index=True)\n",
    "            \n",
    "            else:\n",
    "            # Otherwise, update the existing row\n",
    "                gap_values.loc[row_id, f'{index}_gs'] = gap.values[0]\n",
    "                gap_values.loc[row_id, f'{index}_s'] = s\n",
    "\n",
    "    return gap_values\n",
    "\n",
    "\n",
    "# Select the optimal number of clusters for a given validity index\n",
    "def get_best_gap(index, n_min, n_max):\n",
    "    gap = gap_values[f'{index}_gs']\n",
    "    s = gap_values[f'{index}_s']\n",
    "\n",
    "    stats = []\n",
    "    for i in range(n_min-1, n_max-2):\n",
    "        stat = gap[i] - gap[i+1] + s[i+1]\n",
    "        # Select rows such that GS(k) >= GS(k+1) + s(k+1)\n",
    "        if stat >= 0: \n",
    "            stats.append([i+1, stat])\n",
    "\n",
    "    # Find min value\n",
    "    stats = np.array(stats)\n",
    "    if stats.size == 0:\n",
    "        best_n = 'none'\n",
    "    else:\n",
    "        best_n = int(stats[np.argmin(stats[:, 1]), 0])\n",
    "\n",
    "    return best_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2531313e-b729-48d6-9528-7c3ae22dcb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min = 1\n",
    "n_max = 12\n",
    "iters = 5\n",
    "model = 'LPA'\n",
    "params = 'without covariates'\n",
    "\n",
    "gap_values = compute_gap_statistic(data_n, n_min, n_max, iters, model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6873167a-200a-4015-b2bf-8d507a8c999b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>silhouette</th>\n",
       "      <th>calinski_harabasz</th>\n",
       "      <th>davies_bouldin</th>\n",
       "      <th>dunn</th>\n",
       "      <th>inertia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LPA</td>\n",
       "      <td>without covariates</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model              params  silhouette  calinski_harabasz  davies_bouldin  \\\n",
       "0   LPA  without covariates          10                  3               7   \n",
       "\n",
       "   dunn  inertia  \n",
       "0     8        5  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_gap = pd.DataFrame()\n",
    "new_row = pd.DataFrame([{'model': model, 'params': params}])\n",
    "best_gap = pd.concat([best_gap, new_row], ignore_index=True)\n",
    "\n",
    "for index in val_indexes:\n",
    "    best_n = get_best_gap(index, n_min, n_max)\n",
    "    best_gap[index] = best_n\n",
    "\n",
    "best_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985153cb-5562-4d16-8b7f-16e1a7c1b4de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18157b36-3a4d-4182-9709-a56b66f4d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min = 2\n",
    "n_max = 12\n",
    "iters = 3\n",
    "model = 'kmeans'\n",
    "params = 'dist = manhattan, link = mean'\n",
    "\n",
    "gs = compute_gap_statistic(data_n, n_min, n_max, iters, model, params)\n",
    "gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d20eb39-53e4-48dd-b06b-ef4651c3c0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min = 2\n",
    "n_max = 12\n",
    "n_replicates = 3\n",
    "model = 'AHC'\n",
    "params = 'dist = manhattan, link = average'\n",
    "\n",
    "gs = compute_gap_statistic(data_n, n_min, n_max, n_replicates, model, params)\n",
    "gs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3337c752-a230-407e-a697-be3302458166",
   "metadata": {},
   "source": [
    "# Legacy 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde74a0f-f326-4cdc-9280-fbee5b6d1432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate reference data from a uniform distribution\n",
    "def gen_ref_data(data):\n",
    "    return np.random.uniform(low=data.min(axis=0), \n",
    "                            high=data.max(axis=0), \n",
    "                            size=data.shape)\n",
    "\n",
    "# To retrieve parameters from the models list\n",
    "def extract_params(str):\n",
    "    pairs = [pair.strip() for pair in str.split(',')]\n",
    "    params = {}\n",
    "    for pair in pairs:\n",
    "        key, value = [x.strip() for x in pair.split('=')]\n",
    "        params[key] = value  \n",
    "    return params\n",
    "\n",
    "# Compute the Gap Statistic\n",
    "def compute_gap_statistic(data, n_min, n_max, n_replicates, model, params):\n",
    "    gap_values = []\n",
    "    \n",
    "    # Loop over n values\n",
    "    for n in range(n_min, n_max + 1):\n",
    "    \n",
    "        # Compute the average inertia for the random datasets\n",
    "        sim_inertia = []\n",
    "        \n",
    "        for _ in range(n_replicates):\n",
    "            rand_data = gen_ref_data(data)\n",
    "            \n",
    "            if model == 'LPA' and params == 'without covariates':\n",
    "                rand_inertia = float(do_StepMix(n, 'continuous', rand_data)['silhouette'])\n",
    "            \n",
    "            if model == 'kmeans':\n",
    "                config = extract_params(params)\n",
    "                rand_inertia = float(do_kmeans(rand_data, n, **config)['silhouette'])\n",
    "                \n",
    "            if model == 'AHC':\n",
    "                config = extract_params(params)\n",
    "                rand_inertia = float(do_AHC(rand_data, n, **config)['silhouette'])\n",
    "            \n",
    "            sim_inertia.append(rand_inertia)\n",
    "\n",
    "        # Retrive inertia for the assessed model\n",
    "        mod_inertia = all_models.loc[(all_models['model'] == model) & \n",
    "                                     (all_models['params'] == params) & \n",
    "                                     (all_models['n_clust'] == n), 'silhouette'].values\n",
    "\n",
    "        # Calculate the Gap statistic and s value\n",
    "        gap = np.log(np.mean(sim_inertia)) - np.log(mod_inertia)\n",
    "        s = np.std(np.log(sim_inertia)) * np.sqrt(1 + (1 / n_replicates))\n",
    "        gap_values.append([n, gap.item(), s.item()])\n",
    "\n",
    "    return gap_values\n",
    "\n",
    "# Select the optimal number of clusters\n",
    "def get_best_gap(df):\n",
    "    stats = []\n",
    "    for i in range(0,4):\n",
    "        stat = df[i][1] - df[i-1][1] + df[i][2]\n",
    "        # Select rows such that GS(k) >= GS(k+1) + s(k+1)\n",
    "        if stat >= 0: \n",
    "            stats.append([i+1, stat])\n",
    "\n",
    "    # Find min value\n",
    "    stats = np.array(stats)\n",
    "    best_n = int(stats[np.argmin(stats[:, 1]), 0])\n",
    "\n",
    "    return best_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45e72d0-303f-42ad-b2ad-24bf5d8ecf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min = 5\n",
    "n_max = 8\n",
    "n_replicates = 3\n",
    "model = 'LPA'\n",
    "params = 'without covariates'\n",
    "\n",
    "gs = compute_gap_statistic(data_n, n_min, n_max, n_replicates, model, params)\n",
    "get_best_gap(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ce1274-58b0-4ed8-b70a-1accdd49404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9790509-0ccd-4f25-876d-02de7652c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min = 1\n",
    "n_max = 12\n",
    "n_replicates = 10\n",
    "model = 'kmeans'\n",
    "params = 'dist = manhattan, link = mean'\n",
    "\n",
    "gs = compute_gap_statistic(data_n, n_min, n_max, n_replicates, model, params)\n",
    "get_best_gap(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a31d2d-6f34-48a0-b588-abbf55069456",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min = 1\n",
    "n_max = 12\n",
    "n_replicates = 10\n",
    "model = 'AHC'\n",
    "params = 'dist = euclidean, link = average'\n",
    "\n",
    "gs = compute_gap_statistic(data_n, n_min, n_max, n_replicates, model, params)\n",
    "get_best_gap(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3927d767-6ac3-4cf1-9aea-b18b4e1efe37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d900af75-f47a-43be-99b9-c0f44f788ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4c605c9-c2ab-4043-9931-58d907643c48",
   "metadata": {},
   "source": [
    "# Legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3d0985-5e84-46c9-b12c-9c660272f2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the Gap Statistic\n",
    "def compute_gap_statistic(data, k_min, k_max, n_replicates, model, params):\n",
    "    \n",
    "    # Generate reference data from a uniform distribution\n",
    "    def gen_ref_data(data):\n",
    "        return np.random.uniform(low=data.min(axis=0), \n",
    "                                 high=data.max(axis=0), \n",
    "                                 size=data.shape)\n",
    "\n",
    "    gap_values = []\n",
    "    \n",
    "    # Loop over k values\n",
    "    for k in range(k_min, k_max + 1):\n",
    "        # Retrive inertia for the assessed model\n",
    "        mod_inertia = all_models.loc[(all_models['model'] == model) & (LCA_all['params'] == params) & (LCA_all['n_clust'] == k), 'inertia'].values\n",
    "        \n",
    "        # Compute the average inertia for the reference datasets\n",
    "        reference_inertia = []\n",
    "        for _ in range(n_replicates):\n",
    "            random_data = gen_ref_data(data)\n",
    "            ref_inertia = float(do_StepMix(k, 'continuous', data_n)['inertia'])\n",
    "            reference_inertia.append(ref_inertia)\n",
    "\n",
    "        # Calculate the Gap statistic and se\n",
    "        gap = np.log(np.mean(ref_inertia)) - np.log(mod_inertia)\n",
    "        s = np.std(np.log(sim_inertia)) * np.sqrt(1 + (1 / n_replicates))\n",
    "        gap_values.append([k, float(gap[0]), float(s)])\n",
    "\n",
    "    return gap_values\n",
    "\n",
    "gap_values = compute_gap_statistic(data_n, 1, 8, n_replicates=10, model='LCA', params='without covariates')\n",
    "\n",
    "# Compute GS(k) - GS(k+1) + s(k+1)\n",
    "stats = []\n",
    "for i in range(0,4):\n",
    "    stat = gap_values[i][1] - gap_values[i-1][1] + gap_values[i][2]\n",
    "    # Select rows such that GS(k) >= GS(k+1) + s(k+1)\n",
    "    if stat >= 0: \n",
    "        stats.append([i+1, stat])\n",
    "\n",
    "# Find min value\n",
    "stats = np.array(stats)\n",
    "best_k = int(stats[np.argmin(stats[:, 1]), 0])\n",
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c465f-b844-4758-bc03-dd396adf03f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
